{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find best pose from foundpose results\n",
    "\n",
    "fit from multiview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import dataclass_array as dca\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pycolmap\n",
    "import quaternion\n",
    "import transforms3d as t3d\n",
    "import trimesh\n",
    "import visu3d as v3d\n",
    "import yaml\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"bop_toolkit\")))\n",
    "from bop_toolkit.bop_toolkit_lib.misc import get_symmetry_transformations\n",
    "\n",
    "import burybarrel.colmap_util as cutil\n",
    "from burybarrel.image import render_v3d, render_model, to_contour, imgs_from_dir\n",
    "from burybarrel.plotting import get_axes_traces\n",
    "from burybarrel.camera import load_v3dcams\n",
    "from burybarrel.transform import icp\n",
    "from burybarrel.mesh import segment_pc_from_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_coarse = False\n",
    "use_icp = False\n",
    "# obj_path = Path(\"/scratch/jeyan/barreldata/models3d/depth_charge_mark_9_mod_1-scaled.ply\")\n",
    "obj_path = Path(\"/scratch/jeyan/barreldata/models3d/barrelsingle-scaled.ply\")\n",
    "\n",
    "# camposes_path = Path(\"/scratch/jeyan/barreldata/results/dive3-depthcharge-03-04/cam_poses.json\")\n",
    "# scene_path = Path(\"/scratch/jeyan/barreldata/results/dive3-depthcharge-03-04/openmvs-out/scene_dense_fixed.ply\")\n",
    "# foundpose_res_path = Path(\"/scratch/jeyan/foundpose/output_dive3-depthcharge-03-04/inference/estimated-poses.json\")\n",
    "# imgdir = Path(\"/scratch/jeyan/barreldata/divedata/dive3/dive3-depthcharge-03-04/rgb\")\n",
    "# resdir = Path(\"/scratch/jeyan/barreldata/results/dive3-depthcharge-03-04\")\n",
    "\n",
    "camposes_path = Path(\"/scratch/jeyan/barreldata/divedata/barrel2/cam_poses.json\")\n",
    "scene_path = Path(\"/scratch/jeyan/barreldata/results/barrel2/openmvs-out/scene_dense.ply\")\n",
    "foundpose_res_path = Path(\"/scratch/jeyan/foundpose/output_barrel2/inference/estimated-poses.json\")\n",
    "imgdir = Path(\"/scratch/jeyan/barreldata/divedata/barrel2/rgb\")\n",
    "maskdir = Path(\"/scratch/jeyan/barreldata/results/barrel2/masks\")\n",
    "resdir = Path(\"/scratch/jeyan/barreldata/results/barrel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimeshpc = trimesh.load(scene_path)\n",
    "scenevtxs, scenecols = trimeshpc.vertices, trimeshpc.visual.vertex_colors[:, :3]\n",
    "scenepts = v3d.Point3d(p=scenevtxs, rgb=scenecols)\n",
    "cams, campaths = load_v3dcams(camposes_path, img_parent=imgdir)\n",
    "imgpaths = [imgdir / campath.name for campath in campaths]\n",
    "imgs = [np.array(Image.open(imgpath).convert(\"RGB\")) for imgpath in imgpaths]\n",
    "# camera names are just the filenames without extension\n",
    "camnames = [campath.stem for campath in campaths]\n",
    "name2cam = {camname: cam for camname, cam in zip(camnames, cams)}\n",
    "maskpaths, masks = imgs_from_dir(maskdir, asarray=True, grayscale=True)\n",
    "masks = masks / 255\n",
    "name2mask = {maskpath.stem: mask for maskpath, mask in zip(maskpaths, masks)}\n",
    "# v3d.make_fig([cams, scenepts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(foundpose_res_path, \"rt\") as f:\n",
    "    foundpose_res = json.load(f)\n",
    "# foundpose results are used as reference for image ids\n",
    "name2imgid = {}\n",
    "obj2cams: v3d.Transform = []\n",
    "# camera for each hypothesis. If multiple hypotheses for each image, there will be\n",
    "# duplicate cameras in here\n",
    "camhyps: v3d.Camera = []\n",
    "# colmap usually can't reconstruct every camera pose, so we can only fit the\n",
    "# foundpose results with a camera pose\n",
    "for fres in foundpose_res:\n",
    "    imgpath = Path(fres[\"img_path\"])\n",
    "    name = imgpath.stem\n",
    "    if name not in name2imgid.keys():\n",
    "        name2imgid[name] = fres[\"img_id\"]\n",
    "    # could just use the \"best\" hypothesis. pretty often though, this hypothesis sucks.\n",
    "    # valid_hyp = name in name2cam.keys() and fres[\"hypothesis_id\"] == \"0\"\n",
    "    valid_hyp = name in name2cam.keys()\n",
    "    if valid_hyp:\n",
    "        camhyps.append(name2cam[name])\n",
    "        if use_coarse:\n",
    "            R = fres[\"R_coarse\"]\n",
    "            t = fres[\"t_coarse\"]\n",
    "        else:\n",
    "            R = fres[\"R\"]\n",
    "            t = fres[\"t\"]\n",
    "        T = np.eye(4)\n",
    "        T[:3, :3] = R\n",
    "        T[:3, 3] = np.reshape(t, -1)\n",
    "        obj2cams.append(v3d.Transform.from_matrix(T))\n",
    "obj2cams = dca.stack(obj2cams)\n",
    "camhyps = dca.stack(camhyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load(obj_path)\n",
    "meshvtxs = np.array(mesh.vertices)\n",
    "meshpts = v3d.Point3d(p=meshvtxs, rgb=[255, 0, 0])\n",
    "meshvtxsamp, _ = trimesh.sample.sample_surface_even(mesh, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_T_translation(T: v3d.Transform, scale):\n",
    "    return T.replace(t=T.t * scale)\n",
    "\n",
    "def scale_cams(scale: float, cams: v3d.Camera):\n",
    "    T = cams.world_from_cam\n",
    "    return cams.replace(world_from_cam=scale_T_translation(T, scale))\n",
    "\n",
    "def scale_reconstr(scale: float, cam2worlds: v3d.Transform, obj2cams: v3d.Transform, scenepts: v3d.Point3d):\n",
    "    \"\"\"\n",
    "    Scale a 3D reconstructed scene, its camera positions, and object positions\n",
    "    relative to the camera.\n",
    "\n",
    "    Args:\n",
    "        scale (float): translation scale factor\n",
    "        cam2worlds (nx4x4 Transform): camera to world transforms\n",
    "        obj2cams (nx4x4 Transform): object to camera transforms\n",
    "        scenepts (nx3 Points): scene vertices\n",
    "\n",
    "    Returns:\n",
    "        obj2worlds (nx4x4), cam2worldscaled (nx4x4), scenevtxsscaled (nx3)\n",
    "    \"\"\"\n",
    "    # scaled 3d\n",
    "    cam2worldscaled = scale_T_translation(cam2worlds, scale)\n",
    "    sceneptsscaled = scenepts.replace(p=scenepts.p * scale)\n",
    "    obj2worlds = cam2worldscaled @ obj2cams\n",
    "    return obj2worlds, cam2worldscaled, sceneptsscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac(*data, fit_func=None, loss_func=None, cost_func=None, samp_min=10, inlier_min=10, inlier_thres=0.1, max_iter=1000, seed=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fit_func (data -> model)\n",
    "        loss_func ((model, data) -> array): vectorized loss for individual data points\n",
    "        cost_func ((model, data) -> scalar): total cost to try to minimize\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    best_model = None\n",
    "    best_inlier_idxs = []\n",
    "    best_inliers = []\n",
    "    best_error = float(\"inf\")\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        sample_indices = rng.choice(len(data[0]), samp_min, replace=False)\n",
    "        sample = [singledata[sample_indices] for singledata in data]\n",
    "\n",
    "        model = fit_func(sample)\n",
    "\n",
    "        errors = loss_func(model, data)\n",
    "\n",
    "        inlier_idxs = np.where(errors < inlier_thres)[0]\n",
    "        n_inliers = len(inlier_idxs)\n",
    "        inliers = [singledata[inlier_idxs] for singledata in data]\n",
    "\n",
    "        total_error = cost_func(model, inliers)\n",
    "\n",
    "        if n_inliers >= inlier_min:\n",
    "            if n_inliers > len(best_inlier_idxs) or (n_inliers == len(best_inlier_idxs) and total_error < best_error):\n",
    "                best_model = model\n",
    "                best_inliers = inliers\n",
    "                best_inlier_idxs = inlier_idxs\n",
    "                best_error = total_error\n",
    "    # TODO add condition to relax constraints if this is reached\n",
    "    if best_model is None:\n",
    "        raise ValueError(\"No valid model found after RANSAC\")\n",
    "    return best_model, best_inlier_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this derivative 100% has a closed form but i'm too lazy to solve for it\n",
    "# so screw it, just do gradient descent.\n",
    "def variance_from_scale(scale, data):\n",
    "    camTs = jnp.array(data[0])\n",
    "    objTs = jnp.array(data[1])\n",
    "    scaledcamTs = camTs.at[:, 0:3, 3].multiply(scale)\n",
    "    centershom = scaledcamTs @ objTs @ jnp.array([0, 0, 0, 1.0])\n",
    "    centers = centershom[:, :3]\n",
    "    # trace of cov matrix for now, i guess\n",
    "    return jnp.sum(jnp.var(centers, axis=0))\n",
    "\n",
    "class ScaleCentroidModel():\n",
    "    def __init__(self):\n",
    "        self.scale = None\n",
    "        self.mean = None\n",
    "\n",
    "    def __call__(self, data):\n",
    "        return self.predict(data)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        varfunc_data = lambda x: variance_from_scale(x, data)\n",
    "        grad_cost = grad(varfunc_data)\n",
    "        scaleinit = 1.0\n",
    "        currscale = scaleinit\n",
    "        currgrad = grad_cost(scaleinit)\n",
    "        rate = 0.01\n",
    "        eps = 1e-3\n",
    "        while jnp.abs(currgrad) > eps:\n",
    "            currgrad = grad_cost(currscale)\n",
    "            currscale -= rate * currgrad\n",
    "        self.scale = float(currscale)\n",
    "        centroids = self.predict(data)\n",
    "        self.mean = np.mean(centroids, axis=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, data):\n",
    "        cam2worlds = data[0]\n",
    "        obj2cams = data[1]\n",
    "        scaledcamTs = np.copy(cam2worlds)\n",
    "        scaledcamTs[:, 0:3, 3] *= self.scale\n",
    "        centershom = scaledcamTs @ obj2cams @ jnp.array([0, 0, 0, 1.0])\n",
    "        centers = centershom[:, :3]\n",
    "        return centers\n",
    "\n",
    "# data = (cam2world nx4x4, obj2cam nx4x4)\n",
    "def fitcams(data):\n",
    "    model = ScaleCentroidModel()\n",
    "    model.fit(data)\n",
    "    return model\n",
    "\n",
    "def camloss(model, data):\n",
    "    cents = model(data)\n",
    "    return np.linalg.norm(cents - model.mean, axis=1)\n",
    "\n",
    "def camcost(model, data):\n",
    "    cents = model(data)\n",
    "    return jnp.sum(jnp.var(cents, axis=0))\n",
    "\n",
    "model, inlieridxs = ransac(camhyps.world_from_cam.matrix4x4, obj2cams.matrix4x4, fit_func=fitcams, loss_func=camloss, cost_func=camcost, samp_min=5, inlier_min=5, inlier_thres=0.15, max_iter=50)\n",
    "model.scale, inlieridxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalefactor = model.scale\n",
    "camscaled = scale_cams(scalefactor, cams)\n",
    "obj2worlds, cam2worldscaled, sceneptsscaled = scale_reconstr(scalefactor, camhyps.world_from_cam, obj2cams, scenepts)\n",
    "tofig = []\n",
    "camsinlier = camhyps.replace(world_from_cam=cam2worldscaled)[inlieridxs]\n",
    "obj2worldsinlier: v3d.Transform = obj2worlds[inlieridxs]\n",
    "barrelpts_trf = []\n",
    "tofig.extend([sceneptsscaled, camsinlier])\n",
    "for i, obj2world in enumerate(obj2worldsinlier):\n",
    "    barrelpts_trf.append(obj2world @ meshpts)\n",
    "    tofig.append(barrelpts_trf[-1])\n",
    "v3d.make_fig(*tofig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segidxs = segment_pc_from_masks(sceneptsscaled, masks, camscaled, min_ratio=1/3)\n",
    "meshsamp, _ = trimesh.sample.sample_surface(mesh, count=len(segidxs))\n",
    "objinscenepts = sceneptsscaled[segidxs]\n",
    "tmpT = []\n",
    "if use_icp:\n",
    "    for i, obj2world in enumerate(obj2worldsinlier):\n",
    "        samp_trf = obj2world @ meshsamp\n",
    "        icpT = v3d.Transform.from_matrix(icp(objinscenepts.p, samp_trf))\n",
    "        tmpT.append(icpT.inv @ obj2world)\n",
    "    obj2worldsinlier = dca.stack(tmpT)\n",
    "barrelpts_trf = []\n",
    "tofig.extend([sceneptsscaled, camsinlier])\n",
    "for i, obj2world in enumerate(obj2worldsinlier):\n",
    "    barrelpts_trf.append(obj2world @ meshpts)\n",
    "    tofig.append(barrelpts_trf[-1])\n",
    "v3d.make_fig(*tofig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3d.make_fig(*get_axes_traces(obj2worldsinlier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qangle(q1, q2):\n",
    "    \"\"\"\n",
    "    Angle in radians between 2 quaternions.\n",
    "    https://math.stackexchange.com/questions/3572459/how-to-compute-the-orientation-error-between-two-3d-coordinate-frames\n",
    "    \"\"\"\n",
    "    qerr = q1 * q2.conjugate()\n",
    "    if qerr.w < 0:\n",
    "        qerr *= -1\n",
    "    err = np.atan2(np.sqrt(qerr.x ** 2 + qerr.y ** 2 + qerr.z ** 2), qerr.w)\n",
    "    return err\n",
    "\n",
    "def closest_quat_sym(q1, q2, syms):\n",
    "    \"\"\"\n",
    "    Use q1 as reference, brute force rotate q2 and return that.\n",
    "\n",
    "    Args:\n",
    "        syms (dict): Set of symmetry transformations, each given by a dictionary with:\n",
    "            - 'R': 3x3 ndarray with the rotation matrix.\n",
    "            - 't': 3x1 ndarray with the translation vector.\n",
    "    \"\"\"\n",
    "    errs = []\n",
    "    q2_syms = []\n",
    "    for sym in syms:\n",
    "        q2_sym = q2 * quaternion.from_rotation_matrix(sym[\"R\"])\n",
    "        errs.append(qangle(q1, q2_sym))\n",
    "        q2_syms.append(q2_sym)\n",
    "    return q2_syms[np.argmin(np.abs(errs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(\"/scratch/jeyan/barreldata/models3d/model_info.json\"), \"rt\") as f:\n",
    "    objinfo = json.load(f)\n",
    "symTs = get_symmetry_transformations(objinfo[obj_path.name], 0.01)\n",
    "quatsinlier = quaternion.from_rotation_matrix(obj2worldsinlier.R)\n",
    "ref = quatsinlier[0]\n",
    "quatssymd = [ref]\n",
    "for otherquat in quatsinlier[1:]:\n",
    "    best = closest_quat_sym(ref, otherquat, symTs)\n",
    "    quatssymd.append(best)\n",
    "quatssymd = np.array(quatssymd)\n",
    "obj2worldsinliersym = obj2worldsinlier.replace(R=quaternion.as_rotation_matrix(quatssymd))\n",
    "v3d.make_fig(*get_axes_traces(obj2worldsinliersym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = (n quaternions)\n",
    "def qmean(qs, weights=None):\n",
    "    \"\"\"https://stackoverflow.com/questions/12374087/average-of-multiple-quaternions\"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(qs))\n",
    "    qs = np.squeeze(qs)\n",
    "    Q = quaternion.as_float_array(qs * weights).T\n",
    "    QQ = Q @ Q.T\n",
    "    vals, vecs = np.linalg.eig(QQ)\n",
    "    avg = vecs[:, np.argmax(np.abs(vals))]\n",
    "    avg = avg / np.linalg.norm(avg)\n",
    "    return quaternion.from_float_array(avg)\n",
    "\n",
    "def qloss(model, qs):\n",
    "    qs = np.reshape(qs, -1)\n",
    "    return np.array([qangle(model, q) for q in qs])\n",
    "\n",
    "def qcost(model, qs):\n",
    "    return np.sum(qloss(model, qs))\n",
    "\n",
    "qmeanransac, qinliers = ransac(quatssymd, fit_func=qmean, loss_func=qloss, cost_func=qcost, samp_min=5, inlier_min=5, inlier_thres=0.2, max_iter=50)\n",
    "qmeanransac, qinliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanT = v3d.Transform(R=quaternion.as_rotation_matrix(qmeanransac), t=np.mean(obj2worldsinliersym.t, axis=0))\n",
    "v3d.make_fig(*get_axes_traces(obj2worldsinliersym, scale=0.5), *get_axes_traces(meanT, linewidth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3d.make_fig(camscaled, meanT @ meshpts, sceneptsscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsestr = \"coarse\" if use_coarse else \"refine\"\n",
    "icpstr = \"icp\" if use_icp else \"noicp\"\n",
    "overlaydir = resdir / f\"fit-overlays-{coarsestr}-{icpstr}\"\n",
    "overlaydir.mkdir(exist_ok=True)\n",
    "for i, img in enumerate(imgs):\n",
    "    imgpath = imgpaths[i]\n",
    "    rgb, _, _ = render_model(camscaled[i], mesh, meanT, light_intensity=40.0)\n",
    "    overlayimg = to_contour(rgb, color=(255, 0, 0), background=img)\n",
    "    Image.fromarray(overlayimg).save(overlaydir / f\"{imgpaths[i].stem}.png\")\n",
    "    # Image.fromarray(render_v3d(camscaled[i], meanT @ meshpts, radius=4, background=img)).save(overlaydir / f\"{imgpaths[i].stem}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2camfit = camscaled.world_from_cam.inv @ meanT[..., None]\n",
    "estposes = []\n",
    "for i, obj2cam in enumerate(obj2camfit):\n",
    "    posedata = {\n",
    "        \"img_path\": str(imgpaths[i]),\n",
    "        \"img_id\": name2imgid[camnames[i]],\n",
    "        \"hypothesis_id\": \"0\",\n",
    "        \"R\": obj2cam.R.tolist(),\n",
    "        \"t\": obj2cam.t[..., None].tolist(),\n",
    "    }\n",
    "    estposes.append(posedata)\n",
    "with open(resdir / f\"estimated-poses-{coarsestr}-{icpstr}.json\", \"wt\") as f:\n",
    "    json.dump(estposes, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barrels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
